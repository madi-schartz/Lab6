[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "",
    "text": "# Load in the necessary packages \n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3"
  },
  {
    "objectID": "lab6.html#lab-set-up",
    "href": "lab6.html#lab-set-up",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "",
    "text": "# Load in the necessary packages \n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3"
  },
  {
    "objectID": "lab6.html#data-download",
    "href": "lab6.html#data-download",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Data Download",
    "text": "Data Download\n\n# Load in the data\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n# Documentation PDF \n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 'data/camels_attributes_v2.0.pdf')\n\n# Vector storing data types/files\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n\n# Where we want to download the data ...\n\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n# Download more data\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n# Full join of data\n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab6.html#exploratory-data-analysis",
    "href": "lab6.html#exploratory-data-analysis",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n# Map of q mean of the sites\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"lightblue\", high = \"darkblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab6.html#model-preperation-visual-eda",
    "href": "lab6.html#model-preperation-visual-eda",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Model Preperation : Visual EDA",
    "text": "Model Preperation : Visual EDA\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Test transformation \n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Visualize log transform may benefit q_mean data \n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lab6.html#model-building",
    "href": "lab6.html#model-building",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Model Building",
    "text": "Model Building\n\n# Start by splitting the data \n\nset.seed(123)\n\n# Bad form to perform simple transformations on the outcome variable within a recipe. So, we'll do it here.\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)"
  },
  {
    "objectID": "lab6.html#preprocessor-recipe",
    "href": "lab6.html#preprocessor-recipe",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Preprocessor : recipe",
    "text": "Preprocessor : recipe\n\n# Create a recipe to preprocess the data\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n## Naive base lm approach \n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Correctly evaluate the model \n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)"
  },
  {
    "objectID": "lab6.html#model-evaluation-statistical-and-visual",
    "href": "lab6.html#model-evaluation-statistical-and-visual",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Model Evaluation : Statistical and Visual",
    "text": "Model Evaluation : Statistical and Visual\n\n# Evaluate model using metrics function \n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n# Creating the linear model \n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")"
  },
  {
    "objectID": "lab6.html#workflow",
    "href": "lab6.html#workflow",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Workflow",
    "text": "Workflow\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01"
  },
  {
    "objectID": "lab6.html#making-predictions",
    "href": "lab6.html#making-predictions",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Making Predictions",
    "text": "Making Predictions\n\n# predictions using augment \n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61"
  },
  {
    "objectID": "lab6.html#model-evaluation-statistical-and-visual-1",
    "href": "lab6.html#model-evaluation-statistical-and-visual-1",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Model Evaluation : Statistical and Visual",
    "text": "Model Evaluation : Statistical and Visual\n\n# use metrics function to extract metrics\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n# create scatter plot of the observed vs predicted values, colored by aridity, to visualize the model performance.\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()"
  },
  {
    "objectID": "lab6.html#build-a-xgboost-engine-regression-mode-model-using-boost_tree",
    "href": "lab6.html#build-a-xgboost-engine-regression-mode-model-using-boost_tree",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Build a xgboost (engine) regression (mode) model using boost_tree",
    "text": "Build a xgboost (engine) regression (mode) model using boost_tree\n\nxgBoost_model &lt;- boost_tree(mode = \"regression\",\n                            trees = 1000) |&gt;\n  set_engine('xgboost')"
  },
  {
    "objectID": "lab6.html#build-a-neural-network-model-using-the-nnet-engine-from-the-baguette-package-using-the-bag_mlp-function",
    "href": "lab6.html#build-a-neural-network-model-using-the-nnet-engine-from-the-baguette-package-using-the-bag_mlp-function",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Build a neural network model using the nnet engine from the baguette package using the bag_mlp function",
    "text": "Build a neural network model using the nnet engine from the baguette package using the bag_mlp function\n\nNeuralNet_Model &lt;- bag_mlp(mode = \"regression\") |&gt;\n  set_engine('nnet')"
  },
  {
    "objectID": "lab6.html#add-this-to-the-above-workflow",
    "href": "lab6.html#add-this-to-the-above-workflow",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Add this to the above workflow",
    "text": "Add this to the above workflow\n\nxgbm_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(xgBoost_model) |&gt;\n  fit(data = camels_train) |&gt;\n  augment(camels_train)\n  \nNeuralNet_Model_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(xgBoost_model) |&gt;\n  fit(data = camels_train) |&gt;\n  augment(camels_train)"
  },
  {
    "objectID": "lab6.html#evaluate-the-model-and-compare-it-to-the-linear-and-random-forest-models",
    "href": "lab6.html#evaluate-the-model-and-compare-it-to-the-linear-and-random-forest-models",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Evaluate the model and compare it to the linear and random forest models",
    "text": "Evaluate the model and compare it to the linear and random forest models\n\n# use metrics function to extract metrics\n\nmetrics(xgbm_wf, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     0.00292\n2 rsq     standard     1.00   \n3 mae     standard     0.00211\n\nmetrics(NeuralNet_Model_wf, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     0.00292\n2 rsq     standard     1.00   \n3 mae     standard     0.00211\n\n# build the plots to compare \n\nggplot(xgbm_wf, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(NeuralNet_Model_wf, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "lab6.html#which-of-the-4-models-would-you-move-forward-with",
    "href": "lab6.html#which-of-the-4-models-would-you-move-forward-with",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Which of the 4 models would you move forward with?",
    "text": "Which of the 4 models would you move forward with?\nI would probably move forward with either the random forest model since it was outperforming the linear regression model in the workflow rank, or I would do the boosted tree model as the results are right on the 1:1 line."
  },
  {
    "objectID": "lab6.html#data-splitting",
    "href": "lab6.html#data-splitting",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Data Splitting",
    "text": "Data Splitting\n\n# set a seed for reproducible\n\nset.seed(123)\n\n# Create an initial split with 75% used for training and 25% for testing\n\nresample_split &lt;- initial_split(camels, prop = 0.75)\n\n# Extract your training and testing sites\n\ntrain_camels &lt;- training(resample_split)\nglimpse(train_camels)\n\nRows: 503\nColumns: 59\n$ gauge_id             &lt;chr&gt; \"07184000\", \"08164000\", \"02430085\", \"10172800\", \"…\n$ p_mean               &lt;dbl&gt; 3.255383, 2.933001, 4.232671, 1.943691, 3.589425,…\n$ pet_mean             &lt;dbl&gt; 2.741906, 4.743906, 2.947196, 3.446788, 2.253250,…\n$ p_seasonality        &lt;dbl&gt; 0.405783301, 0.095341135, -0.165291650, -0.353639…\n$ frac_snow            &lt;dbl&gt; 0.045357618, 0.001380127, 0.012676724, 0.47283123…\n$ aridity              &lt;dbl&gt; 0.8422683, 1.6174241, 0.6962971, 1.7733215, 0.627…\n$ high_prec_freq       &lt;dbl&gt; 23.25, 23.75, 22.85, 24.70, 14.65, 20.95, 22.30, …\n$ high_prec_dur        &lt;dbl&gt; 1.332378, 1.503165, 1.248634, 1.403409, 1.181452,…\n$ high_prec_timing     &lt;chr&gt; \"jja\", \"mam\", \"djf\", \"mam\", \"jja\", \"jja\", \"jja\", …\n$ low_prec_freq        &lt;dbl&gt; 271.80, 280.05, 264.65, 277.15, 199.35, 254.35, 2…\n$ low_prec_dur         &lt;dbl&gt; 5.592593, 7.292969, 4.816197, 7.061146, 2.982049,…\n$ low_prec_timing      &lt;chr&gt; \"djf\", \"mam\", \"son\", \"jja\", \"jja\", \"son\", \"djf\", …\n$ geol_1st_class       &lt;chr&gt; \"Mixed sedimentary rocks\", \"Unconsolidated sedime…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.5714790, 0.8655454, 1.0000000, 0.3635059, 1.000…\n$ geol_2nd_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Siliciclastic…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.321433977, 0.134454647, 0.000000000, 0.18768450…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.1876845, 0.000…\n$ geol_porostiy        &lt;dbl&gt; 0.1596, 0.2061, 0.2200, 0.1938, 0.2501, 0.1587, 0…\n$ geol_permeability    &lt;dbl&gt; -15.4824, -13.4365, -13.0000, -12.6177, -13.1728,…\n$ soil_depth_pelletier &lt;dbl&gt; 5.9805447, 39.2885953, 1.0000000, 1.5520000, 1.01…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.3486102, 1.5000000, 1.5000000, 0.5773403, 1.500…\n$ soil_porosity        &lt;dbl&gt; 0.4671097, 0.4278973, 0.4313947, 0.4501201, 0.438…\n$ soil_conductivity    &lt;dbl&gt; 0.6178927, 1.2051835, 1.7015119, 1.2954831, 1.649…\n$ max_water_content    &lt;dbl&gt; 0.6326331, 0.6794842, 0.6565674, 0.2482563, 0.631…\n$ sand_frac            &lt;dbl&gt; 17.50598, 45.07884, 46.04384, 35.16330, 42.00856,…\n$ silt_frac            &lt;dbl&gt; 47.39189, 19.83319, 31.71727, 29.61201, 40.38803,…\n$ clay_frac            &lt;dbl&gt; 35.208759, 34.578309, 22.224495, 13.372886, 17.62…\n$ water_frac           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ organic_frac         &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ other_frac           &lt;dbl&gt; 0.008806217, 0.433653453, 0.000000000, 0.00000000…\n$ gauge_lat            &lt;dbl&gt; 37.28125, 28.95998, 34.46667, 40.49633, 41.76645,…\n$ gauge_lon            &lt;dbl&gt; -95.03267, -96.68637, -88.28361, -112.57440, -78.…\n$ elev_mean            &lt;dbl&gt; 280.99, 73.09, 150.01, 2085.87, 628.71, 21.88, 48…\n$ slope_mean           &lt;dbl&gt; 3.48891, 4.02264, 6.72984, 135.90273, 19.77674, 2…\n$ area_gages2          &lt;dbl&gt; 510.84, 2124.00, 40.56, 11.07, 100.10, 447.58, 22…\n$ area_geospa_fabric   &lt;dbl&gt; 514.82, 2120.71, 40.73, 76.51, 101.16, 435.17, 22…\n$ frac_forest          &lt;dbl&gt; 0.0078, 0.0740, 0.8644, 0.0366, 1.0000, 0.7118, 1…\n$ lai_max              &lt;dbl&gt; 1.7159759, 1.5532298, 4.1218155, 1.1695170, 4.996…\n$ lai_diff             &lt;dbl&gt; 1.3821999, 1.0236174, 3.3999854, 0.8071901, 4.461…\n$ gvf_max              &lt;dbl&gt; 0.6164677, 0.5824469, 0.8212477, 0.4426817, 0.885…\n$ gvf_diff             &lt;dbl&gt; 0.34059165, 0.21378870, 0.40814469, 0.18577568, 0…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.7303335, 0.6249006, 0.5720276, 1.0000000, 1.000…\n$ dom_land_cover       &lt;chr&gt; \"    Croplands\", \"    cropland/natural vegetation…\n$ root_depth_50        &lt;dbl&gt; 0.1638200, 0.1729291, 0.2107647, 0.1200000, 0.190…\n$ root_depth_99        &lt;dbl&gt; 1.500000, 1.543336, 2.108013, 1.500000, 2.000000,…\n$ q_mean               &lt;dbl&gt; 0.97727648, 0.60892487, 1.69908346, 1.36826231, 1…\n$ runoff_ratio         &lt;dbl&gt; 0.30020326, 0.20761157, 0.40142112, 0.70395067, 0…\n$ slope_fdc            &lt;dbl&gt; 0.4202704, 0.4737146, 1.4846057, 1.1747324, 1.840…\n$ baseflow_index       &lt;dbl&gt; 0.13737231, 0.18917345, 0.47717513, 0.72337094, 0…\n$ stream_elas          &lt;dbl&gt; 3.0168198, 3.1757904, 0.8339423, 1.4891734, 1.742…\n$ q5                   &lt;dbl&gt; 0.0000000000, 0.0035708023, 0.1809597297, 0.37571…\n$ q95                  &lt;dbl&gt; 4.8851050, 2.2323274, 5.4891118, 4.8622098, 5.812…\n$ high_q_freq          &lt;dbl&gt; 63.400000, 39.100000, 11.600000, 6.700000, 3.5000…\n$ high_q_dur           &lt;dbl&gt; 4.269360, 3.910000, 1.459119, 4.785714, 1.521739,…\n$ low_q_freq           &lt;dbl&gt; 254.90000, 234.00000, 77.75000, 0.00000, 62.45000…\n$ low_q_dur            &lt;dbl&gt; 19.992157, 26.590909, 12.341270, 0.000000, 11.354…\n$ zero_q_freq          &lt;dbl&gt; 0.08939083, 0.00109514, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 206.1500, 182.8500, 150.8000, 235.0000, 155.6000,…\n$ logQmean             &lt;dbl&gt; -0.02298567, -0.49606039, 0.53008896, 0.31354155,…\n\ntest_camels &lt;- testing(resample_split)\nglimpse(test_camels)\n\nRows: 168\nColumns: 59\n$ gauge_id             &lt;chr&gt; \"01013500\", \"01022500\", \"01030500\", \"01057000\", \"…\n$ p_mean               &lt;dbl&gt; 3.126679, 3.608126, 3.274405, 3.570500, 3.467413,…\n$ pet_mean             &lt;dbl&gt; 1.971555, 2.119256, 2.043594, 2.132744, 2.091698,…\n$ p_seasonality        &lt;dbl&gt; 0.18794026, -0.11452959, 0.04735819, 0.07913685, …\n$ frac_snow            &lt;dbl&gt; 0.31344036, 0.24525901, 0.27701840, 0.25115767, 0…\n$ aridity              &lt;dbl&gt; 0.6305587, 0.5873564, 0.6241114, 0.5973236, 0.603…\n$ high_prec_freq       &lt;dbl&gt; 12.95, 20.55, 17.15, 20.35, 15.85, 19.45, 19.60, …\n$ high_prec_dur        &lt;dbl&gt; 1.348958, 1.205279, 1.207746, 1.169540, 1.152727,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"son\", \"son\", \"jja\", \"jja\", \"son\", …\n$ low_prec_freq        &lt;dbl&gt; 202.20, 233.65, 215.60, 239.30, 201.10, 225.55, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.427119, 3.662226, 3.514262, 3.747847, 2.842403,…\n$ low_prec_timing      &lt;chr&gt; \"mam\", \"jja\", \"djf\", \"djf\", \"mam\", \"mam\", \"son\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Acid plutonic…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.8159044, 0.5906582, 0.5733054, 0.4511106, 0.622…\n$ geol_2nd_class       &lt;chr&gt; \"Basic volcanic rocks\", \"Siliciclastic sedimentar…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.1797294524, 0.1646182103, 0.2870100056, 0.40924…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.00000000, 0.00000000, 0.05214009, 0.07743342, 0…\n$ geol_porostiy        &lt;dbl&gt; 0.1714, 0.0710, 0.1178, 0.0251, 0.0109, 0.0600, 0…\n$ geol_permeability    &lt;dbl&gt; -14.7019, -14.2138, -14.4918, -13.9903, -14.1198,…\n$ soil_depth_pelletier &lt;dbl&gt; 7.4047619, 17.4128079, 19.0114144, 3.7591463, 4.0…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.2484079, 1.4918455, 1.4613632, 1.5000000, 1.143…\n$ soil_porosity        &lt;dbl&gt; 0.4611488, 0.4159055, 0.4590910, 0.4041179, 0.439…\n$ soil_conductivity    &lt;dbl&gt; 1.1065225, 2.3750051, 1.2898073, 4.0303923, 1.755…\n$ max_water_content    &lt;dbl&gt; 0.5580548, 0.6262289, 0.6530198, 0.6171002, 0.476…\n$ sand_frac            &lt;dbl&gt; 27.84183, 59.39016, 32.23546, 69.00674, 42.58599,…\n$ silt_frac            &lt;dbl&gt; 55.156940, 28.080937, 51.779182, 22.943292, 43.33…\n$ clay_frac            &lt;dbl&gt; 16.275732, 12.037646, 14.776824, 7.817565, 12.724…\n$ water_frac           &lt;dbl&gt; 5.3766978, 1.2269127, 1.6343449, 0.0000000, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.4087168, 0.0000000, 1.3302776, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.0000000, 0.3584723, 0.0220161, 0.0000000, 1.096…\n$ gauge_lat            &lt;dbl&gt; 47.23739, 44.60797, 45.50097, 44.30399, 44.51172,…\n$ gauge_lon            &lt;dbl&gt; -68.58264, -67.93524, -68.30596, -70.53968, -71.8…\n$ elev_mean            &lt;dbl&gt; 250.31, 92.68, 143.80, 215.59, 450.54, 518.15, 37…\n$ slope_mean           &lt;dbl&gt; 21.64152, 17.79072, 12.79195, 32.68445, 47.54354,…\n$ area_gages2          &lt;dbl&gt; 2252.70, 573.60, 3676.17, 190.92, 195.13, 22.80, …\n$ area_geospa_fabric   &lt;dbl&gt; 2303.95, 620.38, 3676.09, 197.70, 209.50, 24.92, …\n$ frac_forest          &lt;dbl&gt; 0.9063, 0.9232, 0.8782, 0.9415, 0.9963, 1.0000, 0…\n$ lai_max              &lt;dbl&gt; 4.167304, 4.871392, 4.685200, 5.362949, 4.930550,…\n$ lai_diff             &lt;dbl&gt; 3.340732, 3.746692, 3.665543, 4.587077, 4.273607,…\n$ gvf_max              &lt;dbl&gt; 0.8045674, 0.8639358, 0.8585020, 0.9055105, 0.886…\n$ gvf_diff             &lt;dbl&gt; 0.3716482, 0.3377125, 0.3513934, 0.4587420, 0.476…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.8834519, 0.8204934, 0.9752580, 0.5327806, 0.603…\n$ dom_land_cover       &lt;chr&gt; \"    Mixed Forests\", \"    Mixed Forests\", \"    Mi…\n$ root_depth_50        &lt;dbl&gt; NA, 0.2374345, NA, 0.2091862, 0.2134887, 0.190000…\n$ root_depth_99        &lt;dbl&gt; NA, 2.238444, NA, 2.073141, 2.154840, 2.000000, 2…\n$ q_mean               &lt;dbl&gt; 1.6991545, 2.1730621, 1.8201075, 1.8235506, 2.077…\n$ runoff_ratio         &lt;dbl&gt; 0.5434375, 0.6022689, 0.5558590, 0.5107270, 0.599…\n$ slope_fdc            &lt;dbl&gt; 1.5282185, 1.7762798, 1.8711104, 1.5332322, 1.408…\n$ baseflow_index       &lt;dbl&gt; 0.5852260, 0.5544784, 0.5084407, 0.4746775, 0.487…\n$ stream_elas          &lt;dbl&gt; 1.8453242, 1.7027824, 1.3775052, 1.2806809, 1.107…\n$ q5                   &lt;dbl&gt; 0.24110613, 0.20473436, 0.10714920, 0.07816945, 0…\n$ q95                  &lt;dbl&gt; 6.373021, 7.123049, 6.854887, 6.866097, 7.096611,…\n$ high_q_freq          &lt;dbl&gt; 6.10, 3.90, 12.25, 13.85, 9.05, 2.15, 8.25, 5.60,…\n$ high_q_dur           &lt;dbl&gt; 8.714286, 2.294118, 7.205882, 2.429825, 2.154762,…\n$ low_q_freq           &lt;dbl&gt; 41.35, 65.15, 89.25, 82.95, 46.15, 37.25, 65.20, …\n$ low_q_dur            &lt;dbl&gt; 20.170732, 17.144737, 19.402174, 13.825000, 6.888…\n$ zero_q_freq          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hfd_mean             &lt;dbl&gt; 207.25, 166.25, 184.90, 178.05, 187.55, 190.20, 1…\n$ logQmean             &lt;dbl&gt; 0.530130779, 0.776137301, 0.598895589, 0.60078548…\n\n# Build a 10-fold CV dataset as well\n\ncv_folds &lt;- vfold_cv(train_camels, v = 10)\n\ncv_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [452/51]&gt; Fold01\n 2 &lt;split [452/51]&gt; Fold02\n 3 &lt;split [452/51]&gt; Fold03\n 4 &lt;split [453/50]&gt; Fold04\n 5 &lt;split [453/50]&gt; Fold05\n 6 &lt;split [453/50]&gt; Fold06\n 7 &lt;split [453/50]&gt; Fold07\n 8 &lt;split [453/50]&gt; Fold08\n 9 &lt;split [453/50]&gt; Fold09\n10 &lt;split [453/50]&gt; Fold10"
  },
  {
    "objectID": "lab6.html#recipe",
    "href": "lab6.html#recipe",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Recipe",
    "text": "Recipe\n\n#Define a formula you want to use to predict logQmean\n\nformula &lt;- logQmean ~ p_mean + aridity + high_prec_dur\n\n#Describe in words why you are choosing the formula you are. Consult the downloaded PDF for the data to help you make this decision.\n\n# I chose to create a formula using these three variables since we have used them in all of the questions prior and I feel as though they are representative of factors that deals with daily discharge and precipitation. \n\n#Build a recipe that you feel handles the predictors chosen well\n\n# Create a recipe for the formula \n\nrec &lt;- recipe(logQmean ~ aridity + p_mean + high_prec_dur, data = camels_train) %&gt;%\n  # Log transform the predictor variables \n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term \n  step_interact(terms = ~ aridity:p_mean + aridity:high_prec_dur + p_mean:high_prec_dur) %&gt;%\n  # Drop any rows with missing values in the predictors and outcome\n  step_naomit(all_predictors(), all_outcomes())\n\n# Prep\n\nprepared_rec &lt;- prep(rec, training = camels_train)\n\n# Bake\n\ntest_data &lt;- bake(prepared_rec, new_data = NULL)"
  },
  {
    "objectID": "lab6.html#define-3-models",
    "href": "lab6.html#define-3-models",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Define 3 models",
    "text": "Define 3 models\n\n# Define a random forest model using the rand_forest function setting the engine to ranger and the mode to regression\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n\n#Define two other models of your choice (the two I wanted to move forward with in question 3)\n\nxgbm_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nlg_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")"
  },
  {
    "objectID": "lab6.html#workflow-set",
    "href": "lab6.html#workflow-set",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Workflow set ()",
    "text": "Workflow set ()\n\n#Create a workflow object : Add the recipes, models, and fit them to the re-samples \n\nrf_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(rf_model)\nrf_results &lt;- fit_resamples(rf_wf, resamples = cv_folds)\n\nWarning: package 'ranger' was built under R version 4.4.3\n\nxgbm_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(xgbm_model)\nxgbm_results &lt;- fit_resamples(xgbm_wf, resamples = cv_folds)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\nlg_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(lg_model)\nlg_results &lt;- fit_resamples(lg_wf, resamples = cv_folds)"
  },
  {
    "objectID": "lab6.html#evaluation",
    "href": "lab6.html#evaluation",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Evaluation",
    "text": "Evaluation\n\n# Use autoplot and rank_results to compare the models.\n\nwf &lt;- workflow_set(list(rec), list(rf_model, lg_model, xgbm_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.523  0.0224    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.802  0.0219    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.543  0.0238    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.786  0.0249    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    0.555  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.783  0.0216    10 recipe       line…     3\n\n# Describe what model you think is best and why!\n\n# I think that the random forest tree model is the best since it had outperformed the other two models seen by the workflow rank."
  },
  {
    "objectID": "lab6.html#extract-and-evaluate",
    "href": "lab6.html#extract-and-evaluate",
    "title": "Lab 6: Machine Learning in Hyrdology",
    "section": "Extract and Evaluate",
    "text": "Extract and Evaluate\n\n# Build a workflow (not workflow set) with your favorite model, recipe, and training data (use fit)\n\nfinal_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(rf_model) |&gt;\n  fit(data = train_camels)\n\n\n#Use augment to make predictions on the test data\n\nfinal_wf_data &lt;- augment(final_wf, new_data = camels_test)\n\n# Create a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale\n\nggplot(final_wf_data, aes(x = .pred, y = logQmean, colour = logQmean)) +\n  geom_point() + \n  geom_abline(slope = 1, intercept = 0, color = \"blue\", linetype = \"dashed\") +\n  labs(title = \"Observed vs Predicted Values\",\n       x = \"Predicted logQmean\",\n       y = \"Observed logQmean\") +\n  scale_color_gradient2(low = \"maroon\", mid = \"lightblue\", high = \"darkblue\")\n\n\n\n\n\n\n\n# Describe what you think of the result\n\n# I think that the model is pretty accurate given the plot shows the ratio between our observed and predicted values following closely to the 1:1 ratio."
  }
]