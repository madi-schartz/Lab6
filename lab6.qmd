---
title: "Lab 6: Machine Learning in Hyrdology"
subtitle: "Ecosystem Science"
author:
  - name: Madi Schartz
    email: "mmschartz04@gmail.com"
date: "2025-04-04"
format: html
execute: 
  echo: true
---

## Lab Set Up

```{r}
# Load in the necessary packages 

library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(ggplot2)

```

## Data Download 

```{r}
# Load in the data

root  <- 'https://gdex.ucar.edu/dataset/camels/file'

# Documentation PDF 

download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 'data/camels_attributes_v2.0.pdf')

# Vector storing data types/files

types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# Where the files live online ...

remote_files  <- glue('{root}/camels_{types}.txt')

# Where we want to download the data ...

local_files   <- glue('data/camels_{types}.txt')

# Download more data

walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data

camels <- map(local_files, read_delim, show_col_types = FALSE) 

# Full join of data

camels <- power_full_join(camels ,by = 'gauge_id')

```

# Question 1 : From the documentation PDF, report what zero_q_freq represents.

The zero_q_frequency represents the days where Q (daily discharge) is equal to 0 mm/day.  


## Exploratory Data Analysis 

```{r}

# Map of q mean of the sites

ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "lightblue", high = "darkblue") +
  ggthemes::theme_map()

```

# Question 2 : Model Preperation

```{r}
p1 <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = aridity)) +
  scale_color_gradient(low = "lightgreen", high = "darkred") +
  labs(x = "Longitude", y = "Latitude", title = "Aridity Levels Across America") + 
  ggthemes::theme_map()

p2 <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = p_mean)) +
  scale_color_gradient(low = "lightblue", high = "darkblue") +
  labs(x = "Longitude", y = "Latitude", title = "Mean Daily Precipitation Across America") + 
  ggthemes::theme_map()

ggarrange(p1, p2, ncol = 2)
```


## Model Preperation : Visual EDA

```{r}

# Create a scatter plot of aridity vs rainfall
ggplot(camels, aes(x = aridity, y = p_mean)) +
  # Add points colored by mean flow
  geom_point(aes(color = q_mean)) +
  # Add a linear regression line
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  # Apply the viridis color scale
  scale_color_viridis_c() +
  # Add a title, axis labels, and theme (w/ legend on the bottom)
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")

# Test transformation 

ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  # Apply log transformations to the x and y axes
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")

# Visualize log transform may benefit q_mean data 

ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  # Apply a log transformation to the color scale
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        # Expand the legend width ...
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 

```


## Model Building 

```{r}

# Start by splitting the data 

set.seed(123)

# Bad form to perform simple transformations on the outcome variable within a recipe. So, we'll do it here.

camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split

camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

## Preprocessor : recipe

```{r}

# Create a recipe to preprocess the data

rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())

## Naive base lm approach 

# Prepare the data
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)

# Correctly evaluate the model 

test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)

```

## Model Evaluation : Statistical and Visual

```{r}

# Evaluate model using metrics function 

metrics(test_data, truth = logQmean, estimate = lm_pred)

# Creating the linear model 

ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  # Apply a gradient color scale
  scale_color_gradient2(low = "brown", mid = "orange", high = "darkgreen") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```

## Workflow 

```{r}
# Define model
lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
lm_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(lm_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) 

# Extract the model coefficients from the workflow

summary(extract_fit_engine(lm_wf))$coefficients

# From the base implementation
summary(lm_base)$coefficients

```

## Making Predictions 

```{r}

# predictions using augment 

lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)

```

## Model Evaluation : Statistical and Visual

```{r}

# use metrics function to extract metrics

metrics(lm_data, truth = logQmean, estimate = .pred)

# create scatter plot of the observed vs predicted values, colored by aridity, to visualize the model performance.

ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()

```

## A Workflowset approach

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)

rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

# Question 3: Your Turn!

## Build a xgboost (engine) regression (mode) model using boost_tree 
```{r}

xgBoost_model <- boost_tree(mode = "regression",
                            trees = 1000) |>
  set_engine('xgboost')

```

## Build a neural network model using the nnet engine from the baguette package using the bag_mlp function

```{r}

NeuralNet_Model <- bag_mlp(mode = "regression") |>
  set_engine('nnet')

```

## Add this to the above workflow

```{r}

xgbm_wf <- workflow() |>
  add_recipe(rec) |>
  add_model(xgBoost_model) |>
  fit(data = camels_train) |>
  augment(camels_train)
  
NeuralNet_Model_wf <- workflow() |>
  add_recipe(rec) |>
  add_model(xgBoost_model) |>
  fit(data = camels_train) |>
  augment(camels_train)

```

## Evaluate the model and compare it to the linear and random forest models

```{r}
# use metrics function to extract metrics

metrics(xgbm_wf, truth = logQmean, estimate = .pred)

metrics(NeuralNet_Model_wf, truth = logQmean, estimate = .pred)

# build the plots to compare 

ggplot(xgbm_wf, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()

ggplot(NeuralNet_Model_wf, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()

autoplot(wf)


```

## Which of the 4 models would you move forward with?

I would probably move forward with either the random forest model since it was outperforming the linear regression model in the workflow rank, or I would do the boosted tree model as the results are right on the 1:1 line. 

# Question 4 : Build your own

## Data Splitting 

```{r}

# set a seed for reproducible

set.seed(123)

# Create an initial split with 75% used for training and 25% for testing

resample_split <- initial_split(camels, prop = 0.75)

# Extract your training and testing sites

train_camels <- training(resample_split)
glimpse(train_camels)

test_camels <- testing(resample_split)
glimpse(test_camels)

# Build a 10-fold CV dataset as well

cv_folds <- vfold_cv(train_camels, v = 10)

cv_folds

```

## Recipe

```{r}

#Define a formula you want to use to predict logQmean

formula <- logQmean ~ p_mean + aridity + high_prec_dur

#Describe in words why you are choosing the formula you are. Consult the downloaded PDF for the data to help you make this decision.

# I chose to create a formula using these three variables since we have used them in all of the questions prior and I feel as though they are representative of factors that deals with daily discharge and precipitation. 

#Build a recipe that you feel handles the predictors chosen well

# Create a recipe for the formula 

rec <- recipe(logQmean ~ aridity + p_mean + high_prec_dur, data = camels_train) %>%
  # Log transform the predictor variables 
  step_log(all_predictors()) %>%
  # Add an interaction term 
  step_interact(terms = ~ aridity:p_mean + aridity:high_prec_dur + p_mean:high_prec_dur) %>%
  # Drop any rows with missing values in the predictors and outcome
  step_naomit(all_predictors(), all_outcomes())

# Prep

prepared_rec <- prep(rec, training = camels_train)

# Bake

test_data <- bake(prepared_rec, new_data = NULL)

```

## Define 3 models

```{r}
# Define a random forest model using the rand_forest function setting the engine to ranger and the mode to regression

rf_model <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")


#Define two other models of your choice (the two I wanted to move forward with in question 3)

xgbm_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

lg_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

```

## Workflow set ()

```{r}

#Create a workflow object : Add the recipes, models, and fit them to the re-samples 

rf_wf <- workflow() |>
  add_recipe(rec) |>
  add_model(rf_model)
rf_results <- fit_resamples(rf_wf, resamples = cv_folds)

xgbm_wf <- workflow() |>
  add_recipe(rec) |>
  add_model(xgbm_model)
xgbm_results <- fit_resamples(xgbm_wf, resamples = cv_folds)

lg_wf <- workflow() |>
  add_recipe(rec) |>
  add_model(lg_model)
lg_results <- fit_resamples(lg_wf, resamples = cv_folds)

```


## Evaluation 

```{r}

# Use autoplot and rank_results to compare the models.

wf <- workflow_set(list(rec), list(rf_model, lg_model, xgbm_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)

rank_results(wf, rank_metric = "rsq", select_best = TRUE)

# Describe what model you think is best and why!

# I think that the random forest tree model is the best since it had outperformed the other two models seen by the workflow rank. 
```


## Extract and Evaluate 

```{r}
# Build a workflow (not workflow set) with your favorite model, recipe, and training data (use fit)

final_wf <- workflow() |>
  add_recipe(rec) |>
  add_model(rf_model) |>
  fit(data = train_camels)


#Use augment to make predictions on the test data

final_wf_data <- augment(final_wf, new_data = camels_test)

# Create a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale

ggplot(final_wf_data, aes(x = .pred, y = logQmean, colour = logQmean)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, color = "blue", linetype = "dashed") +
  labs(title = "Observed vs Predicted Values",
       x = "Predicted logQmean",
       y = "Observed logQmean") +
  scale_color_gradient2(low = "maroon", mid = "lightblue", high = "darkblue")

# Describe what you think of the result

# I think that the model is pretty accurate given the plot shows the ratio between our observed and predicted values following closely to the 1:1 ratio. 

```

